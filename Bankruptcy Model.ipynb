{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Polish Bankruptcy Modelling\n",
        "\n",
        "## Introduction"
      ],
      "metadata": {
        "id": "GUlY34laD87S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U imbalanced-learn"
      ],
      "metadata": {
        "id": "gPO0XjTnBOZ3",
        "outputId": "8acb9ac8-b54c-44dd-e033-02a431af09f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchdata\n",
            "  Downloading torchdata-0.5.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.5 MB 28.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: imbalanced-learn in /usr/local/lib/python3.7/dist-packages (0.8.1)\n",
            "Collecting imbalanced-learn\n",
            "  Downloading imbalanced_learn-0.9.1-py3-none-any.whl (199 kB)\n",
            "\u001b[K     |████████████████████████████████| 199 kB 48.7 MB/s \n",
            "\u001b[?25hCollecting urllib3>=1.25\n",
            "  Downloading urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 34.6 MB/s \n",
            "\u001b[?25hCollecting portalocker>=2.0.0\n",
            "  Downloading portalocker-2.6.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchdata) (2.23.0)\n",
            "Collecting torch==1.13.0\n",
            "  Downloading torch-1.13.0-cp37-cp37m-manylinux1_x86_64.whl (890.2 MB)\n",
            "\u001b[K     |██████████████████████████████  | 834.1 MB 1.1 MB/s eta 0:00:49tcmalloc: large alloc 1147494400 bytes == 0x397ee000 @  0x7fde728fa615 0x58ead6 0x4f355e 0x4d222f 0x51041f 0x5b4ee6 0x58ff2e 0x510325 0x5b4ee6 0x58ff2e 0x50d482 0x4d00fb 0x50cb8d 0x4d00fb 0x50cb8d 0x4d00fb 0x50cb8d 0x4bac0a 0x538a76 0x590ae5 0x510280 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x58ff2e 0x50c4fc 0x58fd37 0x50ca37 0x5b4ee6 0x58ff2e\n",
            "\u001b[K     |████████████████████████████████| 890.2 MB 6.4 kB/s \n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 557.1 MB 11 kB/s \n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[K     |████████████████████████████████| 849 kB 47.3 MB/s \n",
            "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 21.0 MB 1.6 MB/s \n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 317.1 MB 31 kB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.13.0->torchdata) (4.1.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.0->torchdata) (57.4.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.0->torchdata) (0.38.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.7.3)\n",
            "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.2.0)\n",
            "Collecting imbalanced-learn\n",
            "  Downloading imbalanced_learn-0.9.0-py3-none-any.whl (199 kB)\n",
            "\u001b[K     |████████████████████████████████| 199 kB 58.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.21.6)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchdata) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchdata) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchdata) (3.0.4)\n",
            "Collecting urllib3>=1.25\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 71.6 MB/s \n",
            "\u001b[?25hInstalling collected packages: nvidia-cublas-cu11, urllib3, nvidia-cudnn-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, torch, portalocker, torchdata, imbalanced-learn\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.12.1+cu113\n",
            "    Uninstalling torch-1.12.1+cu113:\n",
            "      Successfully uninstalled torch-1.12.1+cu113\n",
            "  Attempting uninstall: imbalanced-learn\n",
            "    Found existing installation: imbalanced-learn 0.8.1\n",
            "    Uninstalling imbalanced-learn-0.8.1:\n",
            "      Successfully uninstalled imbalanced-learn-0.8.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.13.1+cu113 requires torch==1.12.1, but you have torch 1.13.0 which is incompatible.\n",
            "torchtext 0.13.1 requires torch==1.12.1, but you have torch 1.13.0 which is incompatible.\n",
            "torchaudio 0.12.1+cu113 requires torch==1.12.1, but you have torch 1.13.0 which is incompatible.\u001b[0m\n",
            "Successfully installed imbalanced-learn-0.9.0 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 portalocker-2.6.0 torch-1.13.0 torchdata-0.5.0 urllib3-1.25.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q4gV6ZOgAXtd",
        "outputId": "26c5b61c-0006-4747-e79d-fa8071670798",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)\n",
        "device = torch.device(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00365/data.zip"
      ],
      "metadata": {
        "id": "auVPmHU3qb5Y",
        "outputId": "7fdf41e2-a4c8-41f8-e609-2d7dd06d6339",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-11-29 23:13:06--  https://archive.ics.uci.edu/ml/machine-learning-databases/00365/data.zip\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8834471 (8.4M) [application/x-httpd-php]\n",
            "Saving to: ‘data.zip’\n",
            "\n",
            "data.zip            100%[===================>]   8.42M  13.0MB/s    in 0.6s    \n",
            "\n",
            "2022-11-29 23:13:07 (13.0 MB/s) - ‘data.zip’ saved [8834471/8834471]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip data.zip"
      ],
      "metadata": {
        "id": "FiGH8OXkqmGf",
        "outputId": "21a6e191-a65e-4353-b566-d65a3c17582d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  data.zip\n",
            "  inflating: 1year.arff              \n",
            "  inflating: 2year.arff              \n",
            "  inflating: 3year.arff              \n",
            "  inflating: 4year.arff              \n",
            "  inflating: 5year.arff              \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib \n",
        "import pandas as pd \n",
        "from scipy.io.arff import loadarff\n",
        "\n",
        "\n",
        "dflist = [pd.DataFrame(loadarff(fp)[0]) for fp in pathlib.Path('.').glob('*.arff')]\n",
        "df = pd.concat(dflist, axis=0)\n",
        "df['class'] = df['class'].apply(lambda x: int(x.decode()))\n",
        "df"
      ],
      "metadata": {
        "id": "yNQ9zWm7qo3-",
        "outputId": "6d342e2b-303c-4dec-e1e7-1b803c883892",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          Attr1    Attr2     Attr3    Attr4      Attr5     Attr6     Attr7  \\\n",
              "0      0.200550  0.37951  0.396410  2.04720   32.35100  0.388250  0.249760   \n",
              "1      0.209120  0.49988  0.472250  1.94470   14.78600  0.000000  0.258340   \n",
              "2      0.248660  0.69592  0.267130  1.55480   -1.15230  0.000000  0.309060   \n",
              "3      0.081483  0.30734  0.458790  2.49280   51.95200  0.149880  0.092704   \n",
              "4      0.187320  0.61323  0.229600  1.40630   -7.31280  0.187320  0.187320   \n",
              "...         ...      ...       ...      ...        ...       ...       ...   \n",
              "10168  0.029970  0.66806  0.066243  1.11030 -105.55000  0.029970  0.038888   \n",
              "10169  0.012843  0.49306 -0.160620  0.61898  -24.80100  0.012843  0.012843   \n",
              "10170  0.015092  0.55759 -0.284600  0.48599  -85.57100  0.015092  0.009826   \n",
              "10171 -0.002554  0.47076  0.424010  1.90070    0.95483 -0.002554  0.001785   \n",
              "10172  0.002072  0.94315 -0.134740  0.85607 -119.92000  0.015226  0.002072   \n",
              "\n",
              "          Attr8    Attr9    Attr10  ...    Attr56    Attr57   Attr58  \\\n",
              "0      1.330500  1.13890  0.504940  ...  0.121960  0.397180  0.87804   \n",
              "1      0.996010  1.69960  0.497880  ...  0.121300  0.420020  0.85300   \n",
              "2      0.436950  1.30900  0.304080  ...  0.241140  0.817740  0.76599   \n",
              "3      1.866100  1.05710  0.573530  ...  0.054015  0.142070  0.94598   \n",
              "4      0.630700  1.15590  0.386770  ...  0.134850  0.484310  0.86515   \n",
              "...         ...      ...       ...  ...       ...       ...      ...   \n",
              "10168  0.482740  1.02920  0.322500  ...  0.028377  0.092931  0.97162   \n",
              "10169  0.905900  1.01450  0.446660  ...  0.014247  0.028752  0.98575   \n",
              "10170  0.694880  1.00600  0.387460  ...  0.005971  0.038950  0.99403   \n",
              "10171  1.114400  0.99293  0.524640  ... -0.007122 -0.004869  1.00710   \n",
              "10172  0.059818  1.77490  0.056417  ...  0.031154  0.036720  0.96220   \n",
              "\n",
              "         Attr59   Attr60   Attr61   Attr62  Attr63   Attr64  class  \n",
              "0      0.001924   8.4160   5.1372   82.658  4.4158   7.4277      0  \n",
              "1      0.000000   4.1486   3.2732  107.350  3.4000  60.9870      0  \n",
              "2      0.694840   4.9909   3.9510  134.270  2.7185   5.2078      0  \n",
              "3      0.000000   4.5746   3.6147   86.435  4.2228   5.5497      0  \n",
              "4      0.124440   6.3985   4.3158  127.210  2.8692   7.8980      0  \n",
              "...         ...      ...      ...      ...     ...      ...    ...  \n",
              "10168  0.209820   3.0914   3.9456  192.220  1.8988   3.4199      1  \n",
              "10169  0.160090  48.6660  63.7520   40.071  9.1087   5.1956      1  \n",
              "10170  0.010091  15.0530  11.9640  114.250  3.1948   2.4201      1  \n",
              "10171  0.000000   6.4289   5.7025   64.291  5.6773  25.3990      1  \n",
              "10172  0.132800   4.1618   4.9734  192.510  1.8960   8.9562      1  \n",
              "\n",
              "[43405 rows x 65 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-12042d5b-9030-4bdd-9605-54d3f8ece587\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Attr1</th>\n",
              "      <th>Attr2</th>\n",
              "      <th>Attr3</th>\n",
              "      <th>Attr4</th>\n",
              "      <th>Attr5</th>\n",
              "      <th>Attr6</th>\n",
              "      <th>Attr7</th>\n",
              "      <th>Attr8</th>\n",
              "      <th>Attr9</th>\n",
              "      <th>Attr10</th>\n",
              "      <th>...</th>\n",
              "      <th>Attr56</th>\n",
              "      <th>Attr57</th>\n",
              "      <th>Attr58</th>\n",
              "      <th>Attr59</th>\n",
              "      <th>Attr60</th>\n",
              "      <th>Attr61</th>\n",
              "      <th>Attr62</th>\n",
              "      <th>Attr63</th>\n",
              "      <th>Attr64</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.200550</td>\n",
              "      <td>0.37951</td>\n",
              "      <td>0.396410</td>\n",
              "      <td>2.04720</td>\n",
              "      <td>32.35100</td>\n",
              "      <td>0.388250</td>\n",
              "      <td>0.249760</td>\n",
              "      <td>1.330500</td>\n",
              "      <td>1.13890</td>\n",
              "      <td>0.504940</td>\n",
              "      <td>...</td>\n",
              "      <td>0.121960</td>\n",
              "      <td>0.397180</td>\n",
              "      <td>0.87804</td>\n",
              "      <td>0.001924</td>\n",
              "      <td>8.4160</td>\n",
              "      <td>5.1372</td>\n",
              "      <td>82.658</td>\n",
              "      <td>4.4158</td>\n",
              "      <td>7.4277</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.209120</td>\n",
              "      <td>0.49988</td>\n",
              "      <td>0.472250</td>\n",
              "      <td>1.94470</td>\n",
              "      <td>14.78600</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.258340</td>\n",
              "      <td>0.996010</td>\n",
              "      <td>1.69960</td>\n",
              "      <td>0.497880</td>\n",
              "      <td>...</td>\n",
              "      <td>0.121300</td>\n",
              "      <td>0.420020</td>\n",
              "      <td>0.85300</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.1486</td>\n",
              "      <td>3.2732</td>\n",
              "      <td>107.350</td>\n",
              "      <td>3.4000</td>\n",
              "      <td>60.9870</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.248660</td>\n",
              "      <td>0.69592</td>\n",
              "      <td>0.267130</td>\n",
              "      <td>1.55480</td>\n",
              "      <td>-1.15230</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.309060</td>\n",
              "      <td>0.436950</td>\n",
              "      <td>1.30900</td>\n",
              "      <td>0.304080</td>\n",
              "      <td>...</td>\n",
              "      <td>0.241140</td>\n",
              "      <td>0.817740</td>\n",
              "      <td>0.76599</td>\n",
              "      <td>0.694840</td>\n",
              "      <td>4.9909</td>\n",
              "      <td>3.9510</td>\n",
              "      <td>134.270</td>\n",
              "      <td>2.7185</td>\n",
              "      <td>5.2078</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.081483</td>\n",
              "      <td>0.30734</td>\n",
              "      <td>0.458790</td>\n",
              "      <td>2.49280</td>\n",
              "      <td>51.95200</td>\n",
              "      <td>0.149880</td>\n",
              "      <td>0.092704</td>\n",
              "      <td>1.866100</td>\n",
              "      <td>1.05710</td>\n",
              "      <td>0.573530</td>\n",
              "      <td>...</td>\n",
              "      <td>0.054015</td>\n",
              "      <td>0.142070</td>\n",
              "      <td>0.94598</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.5746</td>\n",
              "      <td>3.6147</td>\n",
              "      <td>86.435</td>\n",
              "      <td>4.2228</td>\n",
              "      <td>5.5497</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.187320</td>\n",
              "      <td>0.61323</td>\n",
              "      <td>0.229600</td>\n",
              "      <td>1.40630</td>\n",
              "      <td>-7.31280</td>\n",
              "      <td>0.187320</td>\n",
              "      <td>0.187320</td>\n",
              "      <td>0.630700</td>\n",
              "      <td>1.15590</td>\n",
              "      <td>0.386770</td>\n",
              "      <td>...</td>\n",
              "      <td>0.134850</td>\n",
              "      <td>0.484310</td>\n",
              "      <td>0.86515</td>\n",
              "      <td>0.124440</td>\n",
              "      <td>6.3985</td>\n",
              "      <td>4.3158</td>\n",
              "      <td>127.210</td>\n",
              "      <td>2.8692</td>\n",
              "      <td>7.8980</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10168</th>\n",
              "      <td>0.029970</td>\n",
              "      <td>0.66806</td>\n",
              "      <td>0.066243</td>\n",
              "      <td>1.11030</td>\n",
              "      <td>-105.55000</td>\n",
              "      <td>0.029970</td>\n",
              "      <td>0.038888</td>\n",
              "      <td>0.482740</td>\n",
              "      <td>1.02920</td>\n",
              "      <td>0.322500</td>\n",
              "      <td>...</td>\n",
              "      <td>0.028377</td>\n",
              "      <td>0.092931</td>\n",
              "      <td>0.97162</td>\n",
              "      <td>0.209820</td>\n",
              "      <td>3.0914</td>\n",
              "      <td>3.9456</td>\n",
              "      <td>192.220</td>\n",
              "      <td>1.8988</td>\n",
              "      <td>3.4199</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10169</th>\n",
              "      <td>0.012843</td>\n",
              "      <td>0.49306</td>\n",
              "      <td>-0.160620</td>\n",
              "      <td>0.61898</td>\n",
              "      <td>-24.80100</td>\n",
              "      <td>0.012843</td>\n",
              "      <td>0.012843</td>\n",
              "      <td>0.905900</td>\n",
              "      <td>1.01450</td>\n",
              "      <td>0.446660</td>\n",
              "      <td>...</td>\n",
              "      <td>0.014247</td>\n",
              "      <td>0.028752</td>\n",
              "      <td>0.98575</td>\n",
              "      <td>0.160090</td>\n",
              "      <td>48.6660</td>\n",
              "      <td>63.7520</td>\n",
              "      <td>40.071</td>\n",
              "      <td>9.1087</td>\n",
              "      <td>5.1956</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10170</th>\n",
              "      <td>0.015092</td>\n",
              "      <td>0.55759</td>\n",
              "      <td>-0.284600</td>\n",
              "      <td>0.48599</td>\n",
              "      <td>-85.57100</td>\n",
              "      <td>0.015092</td>\n",
              "      <td>0.009826</td>\n",
              "      <td>0.694880</td>\n",
              "      <td>1.00600</td>\n",
              "      <td>0.387460</td>\n",
              "      <td>...</td>\n",
              "      <td>0.005971</td>\n",
              "      <td>0.038950</td>\n",
              "      <td>0.99403</td>\n",
              "      <td>0.010091</td>\n",
              "      <td>15.0530</td>\n",
              "      <td>11.9640</td>\n",
              "      <td>114.250</td>\n",
              "      <td>3.1948</td>\n",
              "      <td>2.4201</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10171</th>\n",
              "      <td>-0.002554</td>\n",
              "      <td>0.47076</td>\n",
              "      <td>0.424010</td>\n",
              "      <td>1.90070</td>\n",
              "      <td>0.95483</td>\n",
              "      <td>-0.002554</td>\n",
              "      <td>0.001785</td>\n",
              "      <td>1.114400</td>\n",
              "      <td>0.99293</td>\n",
              "      <td>0.524640</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.007122</td>\n",
              "      <td>-0.004869</td>\n",
              "      <td>1.00710</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>6.4289</td>\n",
              "      <td>5.7025</td>\n",
              "      <td>64.291</td>\n",
              "      <td>5.6773</td>\n",
              "      <td>25.3990</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10172</th>\n",
              "      <td>0.002072</td>\n",
              "      <td>0.94315</td>\n",
              "      <td>-0.134740</td>\n",
              "      <td>0.85607</td>\n",
              "      <td>-119.92000</td>\n",
              "      <td>0.015226</td>\n",
              "      <td>0.002072</td>\n",
              "      <td>0.059818</td>\n",
              "      <td>1.77490</td>\n",
              "      <td>0.056417</td>\n",
              "      <td>...</td>\n",
              "      <td>0.031154</td>\n",
              "      <td>0.036720</td>\n",
              "      <td>0.96220</td>\n",
              "      <td>0.132800</td>\n",
              "      <td>4.1618</td>\n",
              "      <td>4.9734</td>\n",
              "      <td>192.510</td>\n",
              "      <td>1.8960</td>\n",
              "      <td>8.9562</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>43405 rows × 65 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-12042d5b-9030-4bdd-9605-54d3f8ece587')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-12042d5b-9030-4bdd-9605-54d3f8ece587 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-12042d5b-9030-4bdd-9605-54d3f8ece587');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(df.loc[:, 'Attr1': 'Attr64'], \n",
        "                                                    df['class'], test_size=.2, random_state=42, \n",
        "                                                    stratify=df['class'])\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=.1, \n",
        "                                                      random_state=123, stratify=y_train)"
      ],
      "metadata": {
        "id": "MuQwtk-IrNxr"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "3JWFoGmMAXte",
        "outputId": "cba2be76-dd27-45d5-9a62-b22992a51280",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(66134, 20)\n",
            "(8681, 20)\n",
            "33067\n",
            "0.0499942402948969\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import StandardScaler \n",
        "from sklearn.impute import SimpleImputer \n",
        "from imblearn.over_sampling import SVMSMOTE \n",
        "from sklearn.pipeline import make_pipeline \n",
        "from sklearn.feature_selection import f_classif, SelectKBest\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "svm_smote = SVMSMOTE(sampling_strategy='auto', random_state=42, k_neighbors=10)\n",
        "imputer = SimpleImputer() \n",
        "scale = StandardScaler()\n",
        "fs = SelectKBest(f_classif, k=20) \n",
        "data_processing = make_pipeline(imputer, fs, scale) \n",
        "\n",
        "X_train_res = data_processing.fit_transform(X_train) \n",
        "X_train_res, y_train_res = svm_smote.fit_resample(X_train_res, y_train) \n",
        "\n",
        "X_test = data_processing.transform(X_test) \n",
        "X_valid = data_processing.transform(X_valid)\n",
        "\n",
        "print(X_train_res.shape) \n",
        "print(X_test.shape) \n",
        "print(np.sum(y_train_res)) \n",
        "print(np.sum(y_test)/len(y_test))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader \n",
        "import torch \n",
        "\n",
        "\n",
        "class DS(Dataset): \n",
        "  def __init__(self, features, labels): \n",
        "    self.features = torch.tensor(features).type(torch.float).to(device)\n",
        "    self.labels = torch.tensor(labels).type(torch.float).to(device) \n",
        "\n",
        "  def __len__(self): \n",
        "    return len(self.features) \n",
        "  \n",
        "  def __getitem__(self, i): \n",
        "    return self.features[i, :].reshape(1, -1), self.labels[i].reshape(1, -1) \n"
      ],
      "metadata": {
        "id": "QEbL7wxhuaES"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataloader = DataLoader(DS(X_test, y_test), batch_size=64)\n",
        "valid_dataloader = DataLoader(DS(X_valid, y_valid), batch_size=64)"
      ],
      "metadata": {
        "id": "-zGSW8CavHUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_0bzOrXAXtg"
      },
      "outputs": [],
      "source": [
        "import torch \n",
        "from torch import nn \n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class SimpleMultiLayerPerceptron(nn.Module): \n",
        "    def __init__(self, n_inputs, n_hidden_layer1, n_out=1): \n",
        "        super().__init__()\n",
        "        self.relu_stack = nn.Sequential(\n",
        "            nn.Linear(n_inputs, n_hidden_layer1, dtype=torch.float),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(n_hidden_layer1, n_out, dtype=torch.float)\n",
        "        )\n",
        "        \n",
        "    def forward(self, x): \n",
        "        x = self.relu_stack(x)\n",
        "        return x \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8nhy2dqAXtg",
        "outputId": "a2366e9b-4ffd-4e8c-e616-268f16ba298f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SimpleMultiLayerPerceptron(\n",
            "  (layer0): Linear(in_features=64, out_features=30, bias=True)\n",
            "  (layer1): Linear(in_features=30, out_features=10, bias=True)\n",
            "  (layer2): Linear(in_features=10, out_features=5, bias=True)\n",
            "  (final_layer): Linear(in_features=5, out_features=1, bias=True)\n",
            "  (relu): ReLU()\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "from torch.nn import BCEWithLogitsLoss\n",
        "from torch.optim import Adam\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from tqdm import tqdm \n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "\n",
        "net = SimpleMultiLayerPerceptron(20, 30, 1)\n",
        "net.to(device)\n",
        "\n",
        "loss_fn = BCEWithLogitsLoss()\n",
        "optimizer = Adam(net.parameters(), lr=0.05, weight_decay=0.01 ) \n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=2)\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "MAX_EPOCHS = 2\n",
        "CHKPT = 200\n",
        "WIDTH = len(X_train) // BATCH_SIZE + 1\n",
        "\n",
        "train_dataloader = DataLoader(DS(X_train_res, y_train), batch_size=BATCH_SIZE,\n",
        "                              shuffle=True)\n",
        "\n",
        "\n",
        "with tqdm(total=MAX_EPOCHS*WIDTH) as t: \n",
        "  training_losses, validation_losses = [], []\n",
        "  for epoch in range(MAX_EPOCHS): \n",
        "      for it, (batch, labels) in enumerate(train_dataloader): \n",
        "          output = net(batch) \n",
        "          loss = loss_fn(output, labels) \n",
        "\n",
        "          optimizer.zero_grad() \n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          t.update(1)\n",
        "\n",
        "          if (it+1) % CHKPT == 0 or it==WIDTH-1:\n",
        "              loss = loss.item()\n",
        "              training_losses.append(loss)\n",
        "              with torch.no_grad(): \n",
        "                valid_loss = 0\n",
        "                for valid_batch, valid_labels in valid_dataloader: \n",
        "                  valid_output = net(valid_batch) \n",
        "                  valid_loss += loss_fn(valid_output, valid_labels) \n",
        "              validation_losses.append(valid_loss)\n",
        "            \n",
        "      scheduler.step(valid_loss)\n",
        "\n",
        "plt.plot(training_losses, label='Training losses') \n",
        "plt.plot(validation_losses, label='Validation losses') \n",
        "plt.xlabel('Checkpoints') \n",
        "plt.ylabel('BCE Loss') \n",
        "plt.legend()\n",
        "\n",
        "recall, precis = 0, 0\n",
        "no_pos_sample, no_pos_pred = 0,0\n",
        "with torch.no_grad(): \n",
        "    for batch, labels in test_dataloader: \n",
        "        pred = net(batch) \n",
        "        pred = torch.sigmoid(pred)\n",
        "        pred = (pred >= 0.5).type(torch.float)\n",
        "\n",
        "        recall += (pred[labels==1] == labels[labels==1]).type(torch.float).sum().item()\n",
        "        precis += (pred[pred==1] == labels[pred==1]).type(torch.float).sum().item()\n",
        "        no_pos_pred += (pred==1).type(torch.float).sum().item()\n",
        "        no_pos_sample += (labels==1).type(torch.float).sum().item()\n",
        "\n",
        "recall /= no_pos_sample\n",
        "precis /= no_pos_pred\n",
        "print(f\"Recall: {recall: >0.3f} Precision: {precis: >0.3f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uk49dvNuAXti"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": true,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "vscode": {
      "interpreter": {
        "hash": "0689f48586b0afa45e1397f64656af5174ff23a6865b5ccc3d1d8fba8586d107"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}